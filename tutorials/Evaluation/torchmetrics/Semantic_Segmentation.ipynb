{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Semantic Segmentation \n",
    "author: JoÃ£o G. A. Amorim\n",
    "date: October 09, 2022\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/lapix-ufsc/dl-tutorials/blob/main/tutorials/Evaluation/torchmetrics/Semantic_Segmentation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
      "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "!pip install lapixdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchmetrics\n",
    "from lapixdl.evaluation.model import SegmentationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input and output example\n",
    "\n",
    "-> pred: Predictions from model (probabilities, logits or labels) - at this example we will use labels.\n",
    "\n",
    "-> target: Ground truth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.zeros((100, 100), dtype=torch.uint8)\n",
    "pred[:10, :10] = 1\n",
    "pred[40:50, 40:50] = 2\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.zeros((100, 100), dtype=torch.uint8)\n",
    "\n",
    "target[:10, :10] = 1\n",
    "target[80:90, 80:90] = 3\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just create utilies tools, a map between category id and the category name, and the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, {0: 'background', 1: 'A', 2: 'B', 3: 'C'})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {0: \"background\", 1: \"A\", 2: \"B\", 3: \"C\"}\n",
    "\n",
    "num_classes = len(id2label)\n",
    "\n",
    "num_classes, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the prediction and target for demo visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tensors into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_array = pred.numpy()\n",
    "target_array = target.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a cmap and ensure to have just the number of classes size. Also build a handle for the legend of the figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap(\"Pastel1\")\n",
    "\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\n",
    "    name=cmap.name,\n",
    "    colors=[color for idx, color in zip(range(num_classes), cmap.colors)],\n",
    "    N=num_classes,\n",
    ")\n",
    "\n",
    "legend_elements = [\n",
    "    matplotlib.patches.Patch(\n",
    "        color=cmap(idx),\n",
    "        label=f\"{id}: {name}\",\n",
    "    )\n",
    "    for idx, (id, name) in enumerate(id2label.items())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a figure, and plot target and prediction side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x900 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    nrows=1, ncols=2, sharey=True, sharex=True, figsize=(16, 9), dpi=100\n",
    ")\n",
    "\n",
    "ax1.imshow(target_array, cmap=cmap, vmax=max(id2label), vmin=min(id2label))\n",
    "ax1.legend(handles=legend_elements)\n",
    "ax1.axis(\"off\")\n",
    "ax1.set_title(\"Target\")\n",
    "\n",
    "ax2.imshow(pred_array, cmap=cmap, vmax=max(id2label), vmin=min(id2label))\n",
    "ax2.legend(handles=legend_elements)\n",
    "ax2.axis(\"off\")\n",
    "ax2.set_title(\"Prediction\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics with `torchmetrics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For semantic segmentation we can use the `classification` module to compute the desired metrics.\n",
    "\n",
    "\n",
    "\n",
    "The metrics have practically the same configuration parameters (to be sure, check the docs of each metric). They are:\n",
    "\n",
    "- average=\n",
    "  - 'none' : output will be the metric for each category\n",
    "  - 'macro': the metric is calculate for each class separately, and average the metrics across classes (with equal weights for each class).\n",
    "- mdmc_reduce or mdmc_average=\n",
    "  - 'global': will flatten the inputs, and them apply the `average` as usual. \n",
    "  - None: Should be left unchanged if your data is not multi-dimensional multi-class.\n",
    "- ignore_index=\n",
    "  - Integer specifying a target class to ignore.\n",
    "- num_classes=\n",
    "  - Number of classes.\n",
    "- threshold=\n",
    "  - Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the confusion matrix parameter (tp, fp, tn, fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done using `stat_scores`from torch metrics, you can found more at the [official docs](https://torchmetrics.readthedocs.io/en/stable/classification/stat_scores.html). If desires the confusion matrix, check the [official docs of confusion matrix](https://torchmetrics.readthedocs.io/en/stable/classification/confusion_matrix.html?highlight=confusion%20matrix).\n",
    "\n",
    "The `reduce` parameter will define the reduction which will be applied. The \"macro\" value will compute the statistics for each class separately.\n",
    "\n",
    "The `mdmc_reduce` will be **required** because we are working with a tensor which represent an image. The \"global\" value will flatten the inputs, and them apply the `reduce` as usual. \n",
    "\n",
    "The `num_classes`, the number/quantity of classes is necessary for multicategorical data.\n",
    "\n",
    "\n",
    "\n",
    "With `reduce='macro'` and `mdmc_reduce='global'` the output will be in the shape: (num_classes, 5). Where this 5 values will be `TP`, `FP`, `TN`, `FN`, sup (sup stands for support and equals to: TP + FN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9700,  100,  100,  100, 9800],\n",
       "        [ 100,    0, 9900,    0,  100],\n",
       "        [   0,  100, 9900,    0,    0],\n",
       "        [   0,    0, 9900,  100,  100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat = torchmetrics.functional.stat_scores(\n",
    "    pred, target, reduce=\"macro\", mdmc_reduce=\"global\", num_classes=num_classes\n",
    ")\n",
    "stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a pretty print of this matrix for demonstration only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          \tTP\t | \tFP\t | \tTN\t | \tFN\t | \tsup\n",
      "background |\t9700 \t | \t100  \t | \t100  \t | \t100  \t | \t9800 \n",
      "A          |\t100  \t | \t0    \t | \t9900 \t | \t0    \t | \t100  \n",
      "B          |\t0    \t | \t100  \t | \t9900 \t | \t0    \t | \t0    \n",
      "C          |\t0    \t | \t0    \t | \t9900 \t | \t100  \t | \t100  \n"
     ]
    }
   ],
   "source": [
    "out_sequence = [\"TP\", \"FP\", \"TN\", \"FN\", \"sup\"]\n",
    "\n",
    "print(\" \" * 10 + \"\\t\" + \"\\t | \\t\".join([f\"{t}\" for t in out_sequence]))\n",
    "for idx, name in enumerate(id2label.values()):\n",
    "    txt = \"\\t | \\t\".join([f\"{v:<5}\" for v in stat[idx]])\n",
    "    print(f\"{name:<10} |\\t{txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "[Official docs](https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First needs to init the metric class, and after you can pass the `pred` and `target` tensor to compute the metric values.\n",
    "\n",
    "The output shape in this case will be (num_classes), where each position of the tensor is equals to the IoU of each category. \n",
    "\n",
    "\n",
    "If you desire to have the metric for each category, just use `average='none'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9898, 1.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = torchmetrics.classification.Accuracy(\n",
    "    average=\"none\", mdmc_reduce=\"global\", num_classes=num_classes\n",
    ")\n",
    "acc(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you desire to ignore the value of some specific category, you can use `ignore_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, 1., 0., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = torchmetrics.classification.Accuracy(\n",
    "    average=\"none\", mdmc_reduce=\"global\", num_classes=num_classes, ignore_index=0\n",
    ")\n",
    "acc(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `average='macro'` the metric is calculate for each class separately, and average the metrics across classes (with equal weights for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3333)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = torchmetrics.classification.Accuracy(\n",
    "    average=\"macro\", mdmc_reduce=\"global\", num_classes=num_classes, ignore_index=0\n",
    ")\n",
    "acc(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Index | Intersection over Union (IoU)\n",
    "\n",
    "[Official docs](https://torchmetrics.readthedocs.io/en/stable/classification/jaccard_index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, if you desire to have the metric for each category, just use `average='none'`. \n",
    "\n",
    "First needs to init the metric class, and after you can pass the `pred` and `target` tensor to compute the metric values.\n",
    "\n",
    "The output shape in this case will be (num_classes), where each position of the tensor is equals to the IoU of each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9798, 1.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard = torchmetrics.classification.JaccardIndex(\n",
    "    average=\"none\", num_classes=num_classes\n",
    ")\n",
    "jaccard(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `average='macro'` the metric is calculate for each class separately, and average the metrics across classes (with equal weights for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4949)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard = torchmetrics.classification.JaccardIndex(\n",
    "    average=\"macro\", num_classes=num_classes\n",
    ")\n",
    "jaccard(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice score | F1-score\n",
    "\n",
    "[Official docs f1-score](https://torchmetrics.readthedocs.io/en/stable/classification/f1_score.html)\n",
    "\n",
    "[Official docs dice](https://torchmetrics.readthedocs.io/en/stable/classification/dice.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `average='macro'` the metric is calculate for each class separately, and average the metrics across classes (with equal weights for each class).\n",
    "\n",
    "The `mdmc_reduce` or `mdmc_average` will be **required** because we are working with a tensor which represent an image. The \"global\" value will flatten the inputs, and them apply the `average` as usual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2724)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1score = torchmetrics.classification.F1Score(\n",
    "    average=\"macro\", mdmc_reduce=\"global\", num_classes=num_classes\n",
    ")\n",
    "f1score(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2724)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice = torchmetrics.classification.Dice(\n",
    "    average=\"macro\", mdmc_average=\"global\", num_classes=num_classes\n",
    ")\n",
    "dice(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you desire to have the metric for each category, just use `average='none'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9898, 1.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice = torchmetrics.classification.Dice(\n",
    "    average=\"none\", mdmc_average=\"global\", num_classes=num_classes\n",
    ")\n",
    "\n",
    "dice(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "[Official docs](https://torchmetrics.readthedocs.io/en/stable/classification/precision.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9898, 1.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = torchmetrics.classification.Precision(\n",
    "    average=\"none\", mdmc_average=\"global\", num_classes=num_classes\n",
    ")\n",
    "precision(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "[Official docs](https://torchmetrics.readthedocs.io/en/stable/classification/recall.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9898, 1.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = torchmetrics.classification.Recall(\n",
    "    average=\"none\", mdmc_average=\"global\", num_classes=num_classes\n",
    ")\n",
    "recall(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity\n",
    "\n",
    "[Official docs](https://torchmetrics.readthedocs.io/en/stable/classification/specificity.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 1.0000, 0.9900, 1.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = torchmetrics.classification.Specificity(\n",
    "    average=\"none\", mdmc_average=\"global\", num_classes=num_classes\n",
    ")\n",
    "\n",
    "specificity(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torchmetrics with lapixdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lapixdl` package calculates the confusion matrix first (on the CPU), which this will be slower than calculating using `torchmetrics` which uses pytorch tensors. So a trick here, to not calculate each metric separately in `torchmetrics`, is to calculate a confusion matrix using `torchmetrics` and then calculate all the metrics at once using `lapixdl`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, compute the confusion matrix with torch metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9700,    0,  100,    0],\n",
       "       [   0,  100,    0,    0],\n",
       "       [   0,    0,    0,    0],\n",
       "       [ 100,    0,    0,    0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confMat = torchmetrics.ConfusionMatrix(\n",
    "    reduce=\"macro\", mdmc_reduce=\"global\", num_classes=num_classes\n",
    ")\n",
    "\n",
    "confusion_matrix = confMat(pred, target)\n",
    "confusion_matrix = confusion_matrix.numpy()\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compute the metrics with lapixdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = SegmentationMetrics(\n",
    "    classes=list(id2label.values()), confusion_matrix=confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f55808cd-6b32-4a2c-80ea-b9a76ac405c9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average</th>\n",
       "      <th>background</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.663265</td>\n",
       "      <td>0.9897959183673469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No positive cases in GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.497449</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.872500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F-Score</th>\n",
       "      <td>0.497449</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR</th>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IoU</th>\n",
       "      <td>0.494949</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IoU w/o Background</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9700</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>9900</td>\n",
       "      <td>9900</td>\n",
       "      <td>9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f55808cd-6b32-4a2c-80ea-b9a76ac405c9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f55808cd-6b32-4a2c-80ea-b9a76ac405c9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f55808cd-6b32-4a2c-80ea-b9a76ac405c9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                     Average          background     A     B  \\\n",
       "Accuracy            0.980000                0.98   1.0  0.99   \n",
       "Recall              0.663265  0.9897959183673469   1.0   0.0   \n",
       "Precision           0.497449            0.989796     1   NaN   \n",
       "Specificity         0.872500                 0.5   1.0   1.0   \n",
       "F-Score             0.497449            0.989796   1.0   0.0   \n",
       "FPR                 0.127500                 0.5   0.0   0.0   \n",
       "IoU                 0.494949            0.979798   1.0   0.0   \n",
       "IoU w/o Background  0.333333                 NaN   NaN   NaN   \n",
       "TP                       NaN                9700   100     0   \n",
       "TN                       NaN                 100  9900  9900   \n",
       "FP                       NaN                 100     0     0   \n",
       "FN                       NaN                 100     0   100   \n",
       "\n",
       "                                          C  \n",
       "Accuracy                               0.99  \n",
       "Recall              No positive cases in GT  \n",
       "Precision                               0.0  \n",
       "Specificity                            0.99  \n",
       "F-Score                                 0.0  \n",
       "FPR                                    0.01  \n",
       "IoU                                     0.0  \n",
       "IoU w/o Background                      NaN  \n",
       "TP                                        0  \n",
       "TN                                     9900  \n",
       "FP                                      100  \n",
       "FN                                        0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.to_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

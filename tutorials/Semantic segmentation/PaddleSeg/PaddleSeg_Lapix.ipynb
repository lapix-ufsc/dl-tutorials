{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #Run this cell only once per notebook instance.\n",
    "\n",
    "# @markdown This cell is responsible for installing the Paddle base framework and the segmentation version called PaddleSeg. After the installation is complete a test script will run, it will download a small dataset and run a neural network for a few iterations to verify that everything was installed successfully.\n",
    "\n",
    "# @markdown You can check the box below to download and extract the cloud dataset. This dataset contains 1223 images that contain the following 6 classes: Sky, Tree, Stratocumuliform, Stratiform, Cirriform and Cumuliform. This dataset is already in the expected format, split into training and validation, and can be used as a reference to adapt your dataset to the expected format for PaddleSeg. At the end of the notebook we have a simple script to convert a dataset to PaddleSeg's format.\n",
    "\n",
    "# @markdown ----\n",
    "download_cloud_dataset = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown ----\n",
    "\n",
    "# @markdown ![](https://drive.google.com/uc?export=view&id=1SRt2rdFuKatHHQDSDLHt23ozbsEsRO0l)\n",
    "\n",
    "!pip install paddlepaddle-gpu\n",
    "import paddle\n",
    "from google.colab import drive\n",
    "\n",
    "paddle.utils.run_check()\n",
    "\n",
    "\n",
    "print(paddle.__version__)\n",
    "\n",
    "!git clone https://github.com/PaddlePaddle/PaddleSeg\n",
    "%cd PaddleSeg\n",
    "!pip install -r requirements.txt\n",
    "!sh tests/run_check_install.sh\n",
    "!python setup.py install\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "%cd '/content/'\n",
    "\n",
    "\n",
    "if download_cloud_dataset:\n",
    "    !gdown 1nuk9mBOAQgaPF9WxnoKDBtGXFh3cUeEH\n",
    "    !unzip '/content/PaddleSegNuvens-ComArvore-1223.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opções Gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #Dataset settings.\n",
    "# @markdown Enter a file path:\n",
    "dataset_root = \"/content/PaddleSegNuvens-ComArvore-1223\"  # @param {type:\"string\"}\n",
    "folder_name_dataset = dataset_root.split(\"/\")[-1]\n",
    "train_path = (\n",
    "    \"/content/PaddleSegNuvens-ComArvore-1223/train-paddle.txt\"  # @param {type:\"string\"}\n",
    ")\n",
    "val_path = (\n",
    "    \"/content/PaddleSegNuvens-ComArvore-1223/val-paddle.txt\"  # @param {type:\"string\"}\n",
    ")\n",
    "num_classes = 6  # @param {type:\"number\"}\n",
    "\n",
    "# @markdown #Mean and standard deviation of the dataset, where r = red, g = green and b = blue.\n",
    "mean_r = 0.37555224  # @param {type:\"number\"}\n",
    "mean_g = 0.47573688  # @param {type:\"number\"}\n",
    "mean_b = 0.51197395  # @param {type:\"number\"}\n",
    "\n",
    "std_r = 0.37555224  # @param {type:\"number\"}\n",
    "std_g = 0.47573688  # @param {type:\"number\"}\n",
    "std_b = 0.51197395  # @param {type:\"number\"}\n",
    "\n",
    "\n",
    "# @markdown #Train settings.\n",
    "batch_size = 4  # @param {type:\"number\"}\n",
    "iters = 80000  # @param {type:\"number\"}\n",
    "\n",
    "\n",
    "# @markdown #Enter the desired size so that the image will be resized to this value. Set the original size so that it does not resize.\n",
    "target_size_x = 512  # @param {type:\"number\"}\n",
    "target_size_y = 512  # @param {type:\"number\"}\n",
    "\n",
    "size_folder_name = f\"{target_size_x}-{target_size_y}\"\n",
    "\n",
    "# @markdown #Transforms. These values add up to both the up and down transformation. So 10 saturation will take the original value and can add or remove up to 10 saturation units.\n",
    "\n",
    "saturation_range = 0.5  # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "contrast_range = 0.20  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "brightness_range = 0.20  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "base = f\"\"\"\n",
    "batch_size: {batch_size}\n",
    "iters: {iters}\n",
    "train_dataset:\n",
    "  type: Dataset\n",
    "  dataset_root: {dataset_root}\n",
    "  train_path: {train_path}\n",
    "  num_classes: {num_classes}\n",
    "  transforms:\n",
    "    - type: Resize\n",
    "      target_size: [{target_size_x}, {target_size_y}]\n",
    "    - type: RandomHorizontalFlip\n",
    "    - type: RandomVerticalFlip    \n",
    "    - type: RandomDistort\n",
    "      brightness_range: {brightness_range}\n",
    "      contrast_range: {contrast_range}\n",
    "      saturation_range: {saturation_range}\n",
    "    - type: Normalize\n",
    "      mean: [{mean_r}, {mean_g}, {mean_b}]\n",
    "      std: [{std_r}, {std_g}, {std_b}]\n",
    "  mode: train\n",
    "\n",
    "val_dataset:\n",
    "  type: Dataset\n",
    "  dataset_root: {dataset_root}\n",
    "  val_path: {val_path}\n",
    "  num_classes: {num_classes}\n",
    "  transforms:\n",
    "    - type: Resize\n",
    "      target_size: [{target_size_x}, {target_size_y}]\n",
    "    - type: Normalize\n",
    "      mean: [{mean_r}, {mean_g}, {mean_b}]\n",
    "      std: [{std_r}, {std_g}, {std_b}]\n",
    "  mode: val\n",
    "\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models \n",
    "#Run only one of the cells in this section. If you want to change the experiment, modify and run the cell again, or choose another cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #HRNet\n",
    "# @markdown ---\n",
    "# @markdown #Network Size\n",
    "HrNetSize = \"48\"  # @param [\"18\", \"48\"]\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown #Paper: https://arxiv.org/abs/1909.11065\n",
    "# @markdown #Github: https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.6/configs/ocrnet\n",
    "# @markdown #Overview  We propose a high-resolution network (HRNet). The HRNet maintains high-resolution representations by connecting high-to-low resolution convolutions in parallel and strengthens high-resolution representations by repeatedly performing multi-scale fusions across parallel convolutions. We demonstrate the effectives on pixel-level classification, region-level classification, and image-level classification.\n",
    "\n",
    "\n",
    "# @markdown ![](https://jingdongwang2017.github.io/Projects/HRNet/images/HRNet.jpg)\n",
    "\n",
    "model_folder_name = f\"HrNet-{HrNetSize}\"\n",
    "logits_size = 2\n",
    "model = f\"\"\"\n",
    "\n",
    "model:\n",
    "  type: OCRNet\n",
    "  backbone:\n",
    "    type: HRNet_W{HrNetSize}\n",
    "    pretrained: https://bj.bcebos.com/paddleseg/dygraph/hrnet_w{HrNetSize}_ssld.tar.gz\n",
    "  backbone_indices: [0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #SegFormer\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown #Network size.\n",
    "model_depth = \"B3\"  # @param [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\"]\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown #PPLiteSeg\n",
    "\n",
    "# @markdown #Paper: https://arxiv.org/abs/2105.15203\n",
    "# @markdown #Github: https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.6/configs/segformer\n",
    "\n",
    "\n",
    "# @markdown #Overview:\n",
    "# @markdown SegFormer is a Transformer-based framework for semantic segmentation that unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations.\n",
    "\n",
    "# @markdown ![](https://production-media.paperswithcode.com/methods/c84b18b5-4329-49fc-a5f2-804ef580a966.png)\n",
    "model_depth_down = model_depth.lower()\n",
    "\n",
    "model_folder_name = f\"SegFormer-{model_depth}\"\n",
    "logits_size = 2\n",
    "model = f\"\"\"\n",
    "\n",
    "model:\n",
    "  type: SegFormer_{model_depth}\n",
    "  num_classes: {num_classes}\n",
    "  pretrained: https://bj.bcebos.com/paddleseg/dygraph/mix_vision_transformer_{model_depth_down}.tar.gz\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPLiteSeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #PPLiteSeg\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown #Network size.\n",
    "STDC = 2  # @param {type:\"slider\", min:1, max:2, step:1}\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown #Paper: https://arxiv.org/abs/2204.02681\n",
    "# @markdown #Github: https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.6/configs/pp_liteseg\n",
    "\n",
    "# @markdown #Overview:\n",
    "\n",
    "# @markdown Overview: We propose PP-LiteSeg, a novel lightweight model for the real-time semantic segmentation task. Specifically, we present a Flexible and Lightweight Decoder (FLD) to reduce computation overhead of previous decoder. To strengthen feature representations, we propose a Unified Attention Fusion Module (UAFM), which takes advantage of spatial and channel attention to produce a weight and then fuses the input features with the weight. Moreover, a Simple Pyramid Pooling Module (SPPM) is proposed to aggregate global context with low computation cost.\n",
    "\n",
    "\n",
    "# @markdown ![](https://user-images.githubusercontent.com/52520497/162148786-c8b91fd1-d006-4bad-8599-556daf959a75.png)\n",
    "\n",
    "\n",
    "model_folder_name = f\"PPLiteSeg-{STDC}\"\n",
    "logits_size = 3\n",
    "model = f\"\"\"\n",
    "\n",
    "model:\n",
    "  type: PPLiteSeg\n",
    "  backbone:\n",
    "    type: STDC{STDC}\n",
    "    pretrained: https://bj.bcebos.com/paddleseg/dygraph/PP_STDCNet{STDC}.tar.gz\n",
    "  arm_out_chs: [32, 64, 128]\n",
    "  seg_head_inter_chs: [32, 64, 64]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "# If you want to change the experiment, modify and run the cell again, or choose another cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #AdamW\n",
    "# @markdown #Paper: https://arxiv.org/pdf/1711.05101.pdf\n",
    "# @markdown #API: https://www.paddlepaddle.org.cn/documentation/docs/en/2.2/api/paddle/optimizer/AdamW_en.html#adamw\n",
    "\n",
    "\n",
    "# @markdown #The exponential decay rate for the 1st moment estimates.\n",
    "beta1 = 0.4  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "# @markdown #The exponential decay rate for the 2nd moment estimates.\n",
    "beta2 = 0.984  # @param {type:\"slider\", min:0, max:1, step:0.001}\n",
    "# @markdown #The weight decay coefficient.\n",
    "weight_decay = 0.001  # @param {type:\"number\"}\n",
    "\n",
    "optimizer = f\"\"\"\n",
    "optimizer:\n",
    "  type: AdamW\n",
    "  beta1: {beta1}\n",
    "  beta2: {beta2}\n",
    "  weight_decay: {weight_decay}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #SGD\n",
    "# @markdown #API: https://www.paddlepaddle.org.cn/documentation/docs/en/2.2/api/paddle/optimizer/SGD_en.html#sgd\n",
    "\n",
    "momentum = 0.9  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "weight_decay = 0.0005  # @param {type:\"number\"}\n",
    "\n",
    "optimizer = f\"\"\"\n",
    "optimizer:\n",
    "  type: sgd\n",
    "  momentum: {momentum}\n",
    "  weight_decay: {weight_decay}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate\n",
    "#If you want to change the experiment, modify and run the cell again, or choose another cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #PolynomialDecay\n",
    "\n",
    "learning_rate = 0.9  # @param {type:\"number\"}\n",
    "weight_decay = 0.0005  # @param {type:\"number\"}\n",
    "warmup_iters = 1000  # @param {type:\"number\"}\n",
    "warmup_start_lr_power = 5  # @param {type:\"slider\", min:0, max:6, step:1}\n",
    "\n",
    "\n",
    "lr_scheduler = f\"\"\"\n",
    "\n",
    "lr_scheduler:\n",
    "  type: PolynomialDecay\n",
    "  learning_rate: {learning_rate}\n",
    "  end_lr: 0\n",
    "  power: 0.9\n",
    "  warmup_iters: {warmup_iters}\n",
    "  warmup_start_lr: 1.0e-{warmup_start_lr_power}\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "# If you want to change the experiment, modify and run the cell again, or choose another cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #CrossEntropyLoss\n",
    "# @markdown #Api: https://github.com/PaddlePaddle/PaddleSeg/blob/release/2.6/paddleseg/models/losses/cross_entropy_loss.py\n",
    "\n",
    "# weight_list = '0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.2' #@param {type:\"string\"}\n",
    "\n",
    "folder_name_loss = \"CrossEntropyLoss\"\n",
    "\n",
    "loss = f\"\"\"\n",
    "\n",
    "loss:\n",
    "  types: \n",
    "    - type: MixedLoss\n",
    "      losses:\n",
    "        - type: CrossEntropyLoss\n",
    "      coef: [1]\n",
    "  coef: {[1 for i in list(range(logits_size))]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #DetailAggregateLoss (Single Class)\n",
    "\n",
    "# @markdown #Paper: https://arxiv.org/abs/2104.13188\n",
    "# @markdown #API: https://github.com/PaddlePaddle/PaddleSeg/blob/release/2.6/paddleseg/models/losses/detail_aggregate_loss.py\n",
    "\n",
    "\n",
    "folder_name_loss = \"DetailAggregateLoss\"\n",
    "cross_entropy_weight = 0.2  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "detail_aggregated_weight = 0.2  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "loss = f\"\"\"\n",
    "\n",
    "loss:\n",
    "  types: \n",
    "    - type: MixedLoss\n",
    "      losses:\n",
    "        - type: CrossEntropyLoss\n",
    "        - type: DetailAggregateLoss\n",
    "      coef: [{cross_entropy_weight}, {detail_aggregated_weight}]\n",
    "  coef: {[1 for i in list(range(logits_size))]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #EdgeAttentionLoss (Single Class)\n",
    "\n",
    "# @markdown #Implements the cross entropy loss function. It only compute the edge part.\n",
    "# @markdown #API: https://github.com/PaddlePaddle/PaddleSeg/blob/release/2.6/paddleseg/models/losses/edge_attention_loss.py\n",
    "\n",
    "folder_name_loss = \"EdgeAttentionLoss\"\n",
    "\n",
    "cross_entropy_weight = 0.8  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "edge_attention_weight = 0.2  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "loss = f\"\"\"\n",
    "\n",
    "loss:\n",
    "  types: \n",
    "    - type: MixedLoss\n",
    "      losses:\n",
    "        - type: CrossEntropyLoss\n",
    "        - type: EdgeAttentionLoss\n",
    "      coef: [{cross_entropy_weight}, {edge_attention_weight}]\n",
    "  coef: {[1 for i in list(range(logits_size))]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #PixelContrastCrossEntropyLoss (Arrumar)\n",
    "\n",
    "# @markdown #Paper: https://arxiv.org/abs/2101.11939\n",
    "\n",
    "# @markdown #API: https://github.com/PaddlePaddle/PaddleSeg/blob/release/2.6/paddleseg/models/losses/pixel_contrast_cross_entropy_loss.py\n",
    "\n",
    "folder_name_loss = \"PixelContrastCrossEntropyLoss\"\n",
    "\n",
    "temperature = 0.1  # @param {type:\"number\"}\n",
    "base_temperature = 0.07  # @param {type:\"number\"}\n",
    "max_samples = 1024  # @param {type:\"number\"}\n",
    "max_views = 100  # @param {type:\"number\"}\n",
    "\n",
    "cross_entropy_weight = 0.8  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "pixel_contrast_weight = 0.2  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "\n",
    "loss = f\"\"\"\n",
    "\n",
    "loss:\n",
    "  types:\n",
    "    - type: MixedLoss\n",
    "      losses:\n",
    "        - type: CrossEntropyLoss\n",
    "        - type: PixelContrastCrossEntropyLoss\n",
    "          temperature: {temperature}\n",
    "          base_temperature: {base_temperature}\n",
    "          ignore_index: 255\n",
    "          max_samples: {max_samples}\n",
    "          max_views: {max_views}\n",
    "       coef: [{cross_entropy_weight}, {pixel_contrast_weight}]\n",
    "  coef: {[1 for i in list(range(logits_size))]}\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown #SemanticConnectivityLoss\n",
    "\n",
    "# @markdown #Paper: https://arxiv.org/abs/2112.07146\n",
    "\n",
    "# @markdown #API: https://github.com/PaddlePaddle/PaddleSeg/blob/release/2.6/paddleseg/models/losses/semantic_connectivity_loss.py\n",
    "\n",
    "folder_name_loss = \"SemanticConnectivityLoss\"\n",
    "\n",
    "\n",
    "# @markdown Maximum number of predicted connected components. At the beginning of training, there will be a large number of connected components, and the calculation is very time-consuming. Therefore, it is necessary to limit the maximum number of predicted connected components, and the rest will not participate in the calculation.\n",
    "\n",
    "\n",
    "max_pred_num_conn = 10  # @param {type:\"number\"}\n",
    "\n",
    "cross_entropy_weight = 0.8  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "semantic_connectivity_weight = 0.2  # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "\n",
    "loss = f\"\"\"\n",
    "\n",
    "loss:\n",
    "  types: \n",
    "    - type: MixedLoss\n",
    "      losses:\n",
    "        - type: CrossEntropyLoss\n",
    "        - type: SemanticConnectivityLoss\n",
    "          max_pred_num_conn: {max_pred_num_conn}\n",
    "      coef: [{cross_entropy_weight}, {edge_attention_weight}]\n",
    "  coef: {[1 for i in list(range(logits_size))]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "#@title Run Experiment\n",
    "import os\n",
    "\n",
    "file_content = base + model + optimizer + loss + lr_scheduler \n",
    "\n",
    "\n",
    "experiment_name = os.path.join(folder_name_dataset, model_folder_name, folder_name_loss)\n",
    "resume_model = False\n",
    "save_interval = 200 #@param {type:\"number\"}\n",
    "save_dir_path = '/content/drive/Shareddrives/Nuvens/0Allan/PaddleSegTest' #@param {type:\"string\"}\n",
    "save_dir_exp = os.path.join(save_dir_path, experiment_name)\n",
    "config_file = os.path.join(save_dir_exp, 'config-file.yml')\n",
    "\n",
    "os.makedirs(save_dir_exp, exist_ok=True)\n",
    "\n",
    "#@markdown Resume Experiment?\n",
    "resume_experiment = False #@param {type:\"boolean\"}\n",
    "checkpoint_path = \"/content/drive/Shareddrives/Nuvens/resultados_allan/allan/paddleseg/hrnet18-ocr-comarvore-halfres/iter_9500\" #@param {type:\"string\"}\n",
    "resume_config_file = '/content/drive/Shareddrives/Nuvens/0Allan/PaddleSegTest/PaddleSegNuvens-ComArvore-1223/HrNet-48/DetailAggregateLoss/config-file.yml' #@param {type:\"string\"}\n",
    "\n",
    "with open(config_file, \"w\") as text_file:\n",
    "  text_file.write(file_content)\n",
    "\n",
    "\n",
    "print(config_file)\n",
    "\n",
    "!python /content/PaddleSeg/train.py \\\n",
    "    --config $config_file \\\n",
    "    --do_eval \\\n",
    "    --use_vdl \\\n",
    "    --save_interval $save_interval \\\n",
    "    --save_dir $save_dir_exp\n",
    "\n",
    "if resume_experiment:\n",
    "    print(f'Resuming from {save_dir_exp}')\n",
    "    !python /content/PaddleSeg/train.py \\\n",
    "        --config $resume_config_file \\\n",
    "        --do_eval \\\n",
    "        --use_vdl \\\n",
    "        --save_interval $save_interval \\\n",
    "        --save_dir $save_dir_exp \\\n",
    "        --resume_model $checkpoint_path       \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Predict Folder with trained model\n",
    "\n",
    "\n",
    "#@markdown Files created in the training Experiment\n",
    "checkpoint_path = \"/content/drive/Shareddrives/Nuvens/resultados_allan/allan/paddleseg/hrnet18-ocr-comarvore-halfres/iter_9500\" #@param {type:\"string\"}\n",
    "config_file = '/content/drive/Shareddrives/Nuvens/0Allan/PaddleSegTest/PaddleSegNuvens-ComArvore-1223/HrNet-48/DetailAggregateLoss/config-file.yml' #@param {type:\"string\"}\n",
    "model_params = '/content/drive/Shareddrives/Nuvens/resultados_allan/allan/paddleseg/hrnet18-ocr-comarvore-halfres/best_model/model.pdparams' #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Folder to predict\n",
    "image_folder = '/content/drive/MyDrive/Datasets/2022-05-13' #@param {type:\"string\"}\n",
    "#@markdown Folder to save the predictions\n",
    "dest_folder = '/content/drive/Shareddrives/Nuvens/resultados_allan/allan/paddleseg/hrnet18-ocr-comarvore-halfres/cam1-2022-05-13' #@param {type:\"string\"}\n",
    "#@markdown Custom color pallet, the format is a sequential RGB value for each class, and all values are separated by a space. \n",
    "#@markdown In the example bellow, 0 0 0 is the value for the class zero, 7 25 163 is the value for the class one and so and on.\n",
    "color_pallet = '0 0 0 7 25 163 20 85 189 32 145 215 45 205 241 42 255 49' #@param {type:\"string\"}\n",
    "\n",
    "!python /content/PaddleSeg/predict.py \\\n",
    "       --config $config_file \\\n",
    "       --model_path  \\\n",
    "       --image_path $image_folder \\\n",
    "       --save_dir  $dest_folder \\\n",
    "       --custom_color $color_pallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title String fields\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "mask_ext = \"png\"  # @param {type:\"string\"}\n",
    "image_ext = \"jpg\"  # @param {type:\"string\"}\n",
    "dataset_root = \"/content/PaddleSegNuvens-ComArvore-1223/\"  # @param {type:\"string\"}\n",
    "\n",
    "masks = glob.glob(\n",
    "    os.path.join(Path(dataset_root), \"**\", f\"*.{mask_ext}\"), recursive=True\n",
    ")\n",
    "mask_image_tuple_list = []\n",
    "\n",
    "print(f\"Number of masks found: {len(masks)}\")\n",
    "for mask in tqdm(masks):\n",
    "    file_name = Path(mask).stem\n",
    "    image = glob.glob(\n",
    "        os.path.join(Path(dataset_root), \"**\", f\"{file_name}.{image_ext}\"),\n",
    "        recursive=True,\n",
    "    )[0]\n",
    "    mask_relative = mask.replace(dataset_root, \"\")\n",
    "    image_relative = image.replace(dataset_root, \"\")\n",
    "\n",
    "    mask_image_tuple_list.append((image_relative, mask_relative))\n",
    "\n",
    "validation_percentage = 0.25  # @param {type:\"slider\", min:0.1, max:0.9, step:0.05}\n",
    "\n",
    "X_train, X_test = train_test_split(mask_image_tuple_list, test_size=0.2)\n",
    "\n",
    "train_file = os.path.join(dataset_root, \"train-paddle2.txt\")\n",
    "val_file = os.path.join(dataset_root, \"val-paddle2.txt\")\n",
    "\n",
    "with open(train_file, \"w\") as file:\n",
    "    print(f\"Train Size: {len(X_train)}\")\n",
    "    for line in tqdm(X_train):\n",
    "        file.write(f\"{line[0]} {line[1]}\\n\")\n",
    "\n",
    "with open(val_file, \"w\") as file:\n",
    "    print(f\"Validation Size: {len(X_test)}\")\n",
    "    for line in tqdm(X_test):\n",
    "        file.write(f\"{line[0]} {line[1]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
